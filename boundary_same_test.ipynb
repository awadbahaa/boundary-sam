{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f9161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from boundary_sam.boundary_sam import BoundarySAM\n",
    "from boundary_sam.utilities import get_annotations_for_image,generate_binary_masks,bounding_box, post_process,calculate_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dc410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'# only use gpu otherwise there is no point really. This method works for the huge backbone as we disscussed in the paper.\n",
    "model_size = 'huge'\n",
    "\n",
    "if model_size == 'huge':\n",
    "    WEIGHTS_NAME = \"sam_vit_h_4b8939.pth\"\n",
    "    model_type ='vit_h'\n",
    "elif model_size == 'large':\n",
    "    WEIGHTS_NAME = \"sam_vit_l_0b3195.pth\"\n",
    "    model_type ='vit_l'\n",
    "elif model_size == 'small':\n",
    "    WEIGHTS_NAME = \"sam_vit_b_01ec64.pth\"\n",
    "    model_type ='vit_b'\n",
    "else: \n",
    "    print('model size got to be: huge, large or small')\n",
    "    \n",
    "path_to_weights = '/home/appuser/weights/'# here you put the path where SAM's weights are! \n",
    "sam_checkpoint = f\"{path_to_weights}{WEIGHTS_NAME}\"\n",
    "SAM_ARGS = {'PPS':20,'IoUthresh':0.85,'SST':0.95}\n",
    "\n",
    "bs = BoundarySAM(checkpoint = sam_checkpoint ,model_type = model_type, SAM_ARGS = SAM_ARGS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ac7de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pth_datasets = '/home/appuser/data/'\n",
    "dataset_name = 'konya_cukurava/coco_konya_lowres/'\n",
    "pth_dataset = f'{pth_datasets}{dataset_name}'\n",
    "train_json_pth = f'{pth_dataset}annotations/train2016.json'\n",
    "with open(train_json_pth)  as f:\n",
    "    train_json = json.load(f)\n",
    "train_images_pth = f'{pth_dataset}images/train2016/'\n",
    "\n",
    "im_list = train_json['images']\n",
    "print(len(im_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb12e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_info = im_list[0]\n",
    "im_pth = f'{train_images_pth}{im_info['file_name']}'\n",
    "im = cv2.imread(im_pth)\n",
    "im = cv2.cvtColor(im,cv2.COLOR_BGR2RGB)\n",
    "anns = get_annotations_for_image(im_info['file_name'],train_json)\n",
    "im_width,im_hight,c = im.shape\n",
    "image_size = (im_width,im_hight)\n",
    "masks_gt = generate_binary_masks(anns, image_size)\n",
    "masks_gt = [mask[:,:,0]>0 for mask in masks_gt]\n",
    "masks_gt= [mask for mask in masks_gt   if (np.where(mask==True)[0]==255).sum()==0 and ( np.where(mask==True)[1]==255).sum()==0 and ( np.where(mask==True)[0]==0).sum()==0 and ( np.where(mask==True)[1]==0).sum()==0 ] \n",
    "masks_gt = [mask for mask in masks_gt if bounding_box(mask) is not None]\n",
    "\n",
    "masks_w = bs.generate_masks_original(im)\n",
    "masks_og = [x['segmentation']  for x in masks_w]\n",
    "final_res_og = post_process(masks_w)\n",
    "res_og = 100*np.stack((final_res_og,final_res_og,final_res_og),axis = 2)+im//2\n",
    "plt.imshow(res_og)\n",
    "plt.show()\n",
    "\n",
    "masks_w = bs.generate_masks_enhanced(im)\n",
    "final_res_pca = post_process(masks_w)\n",
    "res_pca = 100*np.stack((final_res_pca,final_res_pca,final_res_pca),axis = 2)+im//2\n",
    "plt.imshow(res_pca)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13910da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MASKS_gt = []\n",
    "MASKS_og = []\n",
    "MASKS_pca =[]\n",
    "for idx, im_info in enumerate(im_list):\n",
    "    im_pth = f'{train_images_pth}{im_info['file_name']}'\n",
    "    im = cv2.imread(im_pth)\n",
    "    im = cv2.cvtColor(im,cv2.COLOR_BGR2RGB)\n",
    "    anns = get_annotations_for_image(im_info['file_name'],train_json)\n",
    "\n",
    "    im_width,im_hight,c = im.shape\n",
    "    image_size = (im_width,im_hight)# Example image size (height, width)\n",
    "    masks_gt = generate_binary_masks(anns, image_size)\n",
    "    masks_gt = [mask[:,:,0]>0 for mask in masks_gt]\n",
    "    masks_gt= [mask for mask in masks_gt   if (np.where(mask==True)[0]==255).sum()==0 and ( np.where(mask==True)[1]==255).sum()==0 and ( np.where(mask==True)[0]==0).sum()==0 and ( np.where(mask==True)[1]==0).sum()==0 ] \n",
    "    masks_gt = [mask for mask in masks_gt if bounding_box(mask) is not None]\n",
    "    \n",
    "\n",
    "    MASKS_gt.append(masks_gt)\n",
    "    \n",
    "\n",
    "    masks_w = bs.generate_masks_original(im)\n",
    "    masks_og = [x['segmentation']  for x in masks_w]\n",
    "    MASKS_og.append(masks_og)\n",
    "    final_res_og = post_process(masks_w)\n",
    "    masks_w = bs.generate_masks_enhanced(im)\n",
    "    masks_pca = [x['segmentation']  for x in masks_w]\n",
    "    MASKS_pca.append(masks_pca)\n",
    "    final_res_pca = post_process(masks_w)\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"{idx + 1}/{len(im_list)}\")\n",
    "    if idx + 1 >= 50:\n",
    "        break\n",
    "    # break\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93575346",
   "metadata": {},
   "outputs": [],
   "source": [
    "IoU_thresh = 0.25\n",
    "metrics_og_konya = []\n",
    "metrics_pca_konya = []\n",
    "for idx, (masks_gt,masks_og,masks_pca) in enumerate(zip(MASKS_gt,MASKS_og,MASKS_pca)):\n",
    "    us_imag_og,os_image_og,fn_ratio_og,iou_image_og = calculate_metrics(masks_gt,masks_og,IoU_thresh=IoU_thresh)\n",
    "    us_imag_pca,os_image_pca,fn_ratio_pca,iou_image_pca =  calculate_metrics(masks_gt,masks_pca,IoU_thresh=IoU_thresh)\n",
    "    metrics_og_konya.append([us_imag_og,os_image_og,fn_ratio_og,iou_image_og])\n",
    "    metrics_pca_konya.append([us_imag_pca,os_image_pca,fn_ratio_pca,iou_image_pca])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc4d748",
   "metadata": {},
   "outputs": [],
   "source": [
    "100*np.mean(abs(np.asarray(metrics_og_konya)),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9035fe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "100*np.mean(abs(np.asarray(metrics_pca_konya)),axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
